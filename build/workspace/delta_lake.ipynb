{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Delta Lake**: Stream Processing with Spark and Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "mkdir -p /opt/workspace/data/loans/\n",
    "curl -o /opt/workspace/data/loans/SAISEU19-loan-risks.snappy.parquet https://pages.databricks.com/rs/094-YMS-629/images/SAISEU19-loan-risks.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"pyspark-delta-lake-notebook\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.executor.memory\", \"512m\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data = spark.read.format(\"parquet\").load(\"data/loans/SAISEU19-loan-risks.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_data.createOrReplaceTempView(\"loans_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from loans_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from loans_parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = \"data/loans/loans_delta\"\n",
    "\n",
    "loans_data.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(delta_path).createOrReplaceTempView(\"loans_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from loans_delta order by loan_id desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_checkpoint_dir(): \n",
    "    return \"/tmp/delta_demo/chkpt/%s\" % str(random.randint(0, 10000))\n",
    "\n",
    "def random_state(states = [\"CA\", \"TX\", \"NY\", \"IA\"]):\n",
    "    return str(random.choice(states))\n",
    "\n",
    "def generate_and_append_data_stream_fixed(table_format, table_path):\n",
    "    stream_data = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 50).load() \\\n",
    "        .withColumn(\"loan_id\", 10000 + col(\"value\")) \\\n",
    "        .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \\\n",
    "        .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \\\n",
    "        .withColumn(\"addr_state\", lit(random_state())) \\\n",
    "        .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\")\n",
    "    query = stream_data.writeStream \\\n",
    "        .format(table_format) \\\n",
    "        .option(\"checkpointLocation\", random_checkpoint_dir()) \\\n",
    "        .trigger(processingTime=\"10 seconds\") \\\n",
    "        .start(table_path)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_query_1 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)\n",
    "stream_query_2 = generate_and_append_data_stream_fixed(table_format = \"delta\", table_path = delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select count(*) from loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def stop_all_streams():\n",
    "    # Stop all the streams\n",
    "    print(\"Stopping all streams...\")\n",
    "    for s in spark.streams.active:\n",
    "        s.stop()\n",
    "    print(\"Stopped all streams.\")\n",
    "    print(\"Deleting checkpoints...\")  \n",
    "    shutil.rmtree(\"/tmp/delta_demo/chkpt/\", True)\n",
    "    print(\"Deleted checkpoints.\")\n",
    "\n",
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
